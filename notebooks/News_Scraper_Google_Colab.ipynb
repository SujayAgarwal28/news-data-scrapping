{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2723cc3",
   "metadata": {},
   "source": [
    "## ⚡ MULTI-SESSION STRATEGY (RECOMMENDED)\n",
    "\n",
    "### **Problem:** Scraping 4.75 years takes 4-6 hours. Colab may disconnect.\n",
    "\n",
    "### **Solution:** Split into 3 sessions!\n",
    "\n",
    "**Session 1:** `START_DATE = (2021, 1, 1)`, `END_DATE = (2022, 6, 30)` (~500 days)  \n",
    "→ Run overnight, download JSON (~10,000 articles)\n",
    "\n",
    "**Session 2:** `START_DATE = (2022, 7, 1)`, `END_DATE = (2024, 1, 1)` (~500 days)  \n",
    "→ Run next night, download JSON (~10,000 articles)\n",
    "\n",
    "**Session 3:** `START_DATE = (2024, 1, 2)`, `END_DATE = (2025, 10, 9)` (~600 days)  \n",
    "→ Run third night, download JSON (~12,000 articles)\n",
    "\n",
    "**Result:** ~30,000+ articles total! 🎯\n",
    "\n",
    "### **On Your PC:**\n",
    "```powershell\n",
    "# Extract all 3 ZIPs to data/raw/news/\n",
    "# Then merge:\n",
    "python src/scraping/2_merge_data.py  # Auto-removes duplicates!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea37d3",
   "metadata": {},
   "source": [
    "# 📰 StockBus News Scraper - Google Colab PRODUCTION Edition\n",
    "\n",
    "**Created:** October 9, 2025  \n",
    "**Updated:** Production version with checkpoint saving  \n",
    "**Purpose:** Scrape 20-30 articles/day for 2021-2025 (30,000+ articles)\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 KEY FEATURES:\n",
    "- ✅ **AUTO-SAVE every 10 articles** - Never lose progress!\n",
    "- ✅ **Resume from crash** - Picks up where it left off\n",
    "- ✅ **25 articles per topic** - More data per scrape\n",
    "- ✅ **6 topics** - Better coverage\n",
    "- ✅ **Download JSONs** - Process on your PC\n",
    "\n",
    "---\n",
    "\n",
    "## \udcca WHAT YOU'LL GET:\n",
    "- **Target:** 20-30 articles/day\n",
    "- **Topics:** 6 Indian stock market topics\n",
    "- **Time Period:** Custom (recommend 2021-2025)\n",
    "- **Total Expected:** 30,000-35,000 articles\n",
    "- **Time:** 4-6 hours (can run overnight!)\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ STRATEGY:\n",
    "1. Run this in **2-3 Colab sessions** (split date ranges)\n",
    "2. Each session scrapes 500-600 days\n",
    "3. Auto-saves every 10 articles (crash-proof!)\n",
    "4. Download JSONs after each session\n",
    "5. Merge on your PC\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b05db",
   "metadata": {},
   "source": [
    "## 1️⃣ Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q gnews==0.4.2\n",
    "!pip install -q selenium==4.35.0\n",
    "!pip install -q webdriver-manager==4.0.2\n",
    "!pip install -q beautifulsoup4==4.12.3\n",
    "!pip install -q newspaper3k==0.2.8\n",
    "!pip install -q lxml==5.3.0\n",
    "!pip install -q lxml_html_clean==0.4.3\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"✅ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4f95c",
   "metadata": {},
   "source": [
    "## 2️⃣ Setup Chrome WebDriver for Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Chrome and ChromeDriver for Colab\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "\n",
    "# Set Chrome options for headless mode\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "\n",
    "print(\"✅ Chrome WebDriver ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d00ae6",
   "metadata": {},
   "source": [
    "## 3️⃣ Create News Scraper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "import hashlib\n",
    "from datetime import datetime, date\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gnews import GNews\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45958e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsScraperColab:\n",
    "    \"\"\"\n",
    "    PRODUCTION Google Colab Scraper\n",
    "    Features:\n",
    "    - Auto-save every 10 articles (crash-proof!)\n",
    "    - Resume from checkpoint\n",
    "    - Cache to avoid duplicates\n",
    "    - Better extraction logic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_interval=10):\n",
    "        self.gnews = None\n",
    "        self.driver = None\n",
    "        self.results = []\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.cache = set()  # URLs already scraped\n",
    "        self.cache_file = 'scraped_cache.json'\n",
    "        self._load_cache()\n",
    "        \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load cache of already-scraped URLs\"\"\"\n",
    "        try:\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                cache_data = json.load(f)\n",
    "                self.cache = set(cache_data)\n",
    "            print(f\"✅ Loaded cache: {len(self.cache)} URLs already scraped\")\n",
    "        except:\n",
    "            print(\"📝 Starting fresh (no cache found)\")\n",
    "            \n",
    "    def _save_cache(self):\n",
    "        \"\"\"Save cache to disk\"\"\"\n",
    "        with open(self.cache_file, 'w') as f:\n",
    "            json.dump(list(self.cache), f)\n",
    "            \n",
    "    def _is_cached(self, url):\n",
    "        \"\"\"Check if URL already scraped\"\"\"\n",
    "        url_hash = hashlib.md5(url.encode()).hexdigest()\n",
    "        return url_hash in self.cache\n",
    "        \n",
    "    def _mark_cached(self, url):\n",
    "        \"\"\"Mark URL as scraped\"\"\"\n",
    "        url_hash = hashlib.md5(url.encode()).hexdigest()\n",
    "        self.cache.add(url_hash)\n",
    "        \n",
    "    def _init_driver(self):\n",
    "        \"\"\"Initialize Chrome driver for Colab\"\"\"\n",
    "        if self.driver:\n",
    "            return\n",
    "            \n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument('--headless')\n",
    "        chrome_options.add_argument('--no-sandbox')\n",
    "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')\n",
    "        \n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        print(\"✅ Chrome driver initialized\")\n",
    "    \n",
    "    def _extract_with_newspaper3k(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Extract article using newspaper3k\"\"\"\n",
    "        try:\n",
    "            article = Article(url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            \n",
    "            if article.text and len(article.text) > 100:\n",
    "                return {\n",
    "                    'title': article.title or 'No title',\n",
    "                    'body': article.text,\n",
    "                    'published_date': article.publish_date.strftime('%d/%m/%Y') if article.publish_date else None,\n",
    "                    'authors': article.authors,\n",
    "                    'extraction_method': 'newspaper3k'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _extract_with_selenium(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Fallback extraction with Selenium + BeautifulSoup\"\"\"\n",
    "        try:\n",
    "            self._init_driver()\n",
    "            self.driver.get(url)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Extract text from paragraphs\n",
    "            paragraphs = soup.find_all('p')\n",
    "            body = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
    "            \n",
    "            if len(body) > 100:\n",
    "                title = soup.find('h1')\n",
    "                return {\n",
    "                    'title': title.get_text().strip() if title else 'No title',\n",
    "                    'body': body,\n",
    "                    'published_date': None,\n",
    "                    'authors': [],\n",
    "                    'extraction_method': 'beautifulsoup'\n",
    "                }\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    def _save_checkpoint(self, articles: List[Dict], topic: str):\n",
    "        \"\"\"Save checkpoint (every N articles)\"\"\"\n",
    "        if not articles:\n",
    "            return\n",
    "            \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"checkpoint_{topic.replace(' ', '_').lower()}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Also save cache\n",
    "        self._save_cache()\n",
    "        \n",
    "        print(f\"💾 Checkpoint saved: {filename} ({len(articles)} articles)\")\n",
    "    \n",
    "    def scrape_topic(self, topic: str, start_date: tuple, end_date: tuple, max_articles: int = 25) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Scrape articles for a topic with checkpoint saving\n",
    "        \n",
    "        Args:\n",
    "            topic: Search topic\n",
    "            start_date: (year, month, day)\n",
    "            end_date: (year, month, day)\n",
    "            max_articles: Max articles to scrape per topic\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"🔍 Topic: {topic}\")\n",
    "        print(f\"📅 Date: {start_date} to {end_date}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Initialize GNews with date range\n",
    "        self.gnews = GNews(\n",
    "            language='en',\n",
    "            country='IN',\n",
    "            max_results=max_articles,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            news_items = self.gnews.get_news(topic)\n",
    "            print(f\"📰 Found {len(news_items)} news items from GNews\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Search failed: {e}\")\n",
    "            return []\n",
    "        \n",
    "        articles = []\n",
    "        saved_count = 0\n",
    "        \n",
    "        for i, item in enumerate(tqdm(news_items, desc=f\"Scraping\", unit=\"item\")):\n",
    "            try:\n",
    "                # Get full article URL\n",
    "                full_url = item.get('url', '')\n",
    "                if not full_url:\n",
    "                    continue\n",
    "                \n",
    "                # Skip if already scraped (resume capability!)\n",
    "                if self._is_cached(full_url):\n",
    "                    print(f\"⏭️  Skipping cached: {full_url[:50]}...\")\n",
    "                    continue\n",
    "                \n",
    "                # Try newspaper3k first\n",
    "                article_data = self._extract_with_newspaper3k(full_url)\n",
    "                \n",
    "                # Fallback to Selenium if needed\n",
    "                if not article_data:\n",
    "                    article_data = self._extract_with_selenium(full_url)\n",
    "                \n",
    "                if article_data:\n",
    "                    # Add metadata\n",
    "                    article_data.update({\n",
    "                        'url': full_url,\n",
    "                        'original_url': item.get('url', ''),\n",
    "                        'scraped_date': datetime.now().strftime('%d/%m/%Y'),\n",
    "                        'topic': topic,\n",
    "                        'publisher': item.get('publisher', {}).get('title', 'Unknown'),\n",
    "                        'gnews_title': item.get('title', ''),\n",
    "                        'body_length': len(article_data['body']),\n",
    "                        'word_count': len(article_data['body'].split())\n",
    "                    })\n",
    "                    \n",
    "                    articles.append(article_data)\n",
    "                    self._mark_cached(full_url)\n",
    "                    \n",
    "                    # CHECKPOINT SAVE (crash-proof!)\n",
    "                    if len(articles) % self.checkpoint_interval == 0:\n",
    "                        self._save_checkpoint(articles, topic)\n",
    "                \n",
    "                time.sleep(1)  # Rate limiting\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        # Final save\n",
    "        if articles:\n",
    "            self._save_checkpoint(articles, topic)\n",
    "        \n",
    "        print(f\"\\n✅ Scraped {len(articles)} articles for '{topic}'\")\n",
    "        return articles\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Close driver and save cache\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            print(\"✅ Chrome driver closed\")\n",
    "        self._save_cache()\n",
    "        print(\"✅ Cache saved\")\n",
    "\n",
    "print(\"✅ NewsScraperColab class created!\")\n",
    "print(\"💾 Auto-saves every 10 articles\")\n",
    "print(\"🔄 Can resume from crash!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5350af",
   "metadata": {},
   "source": [
    "## 4️⃣ Define Topics to Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ec8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics for Indian stock market news (6 topics for better coverage)\n",
    "TOPICS = [\n",
    "    \"Nifty 50 stock market India\",\n",
    "    \"BSE Sensex India stock market\",\n",
    "    \"Indian stock market news\",\n",
    "    \"NSE India trading\",\n",
    "    \"Nifty 50 India today\",\n",
    "    \"Indian stock market today\"\n",
    "]\n",
    "\n",
    "# Date range configuration\n",
    "# RECOMMENDED SPLITS FOR OVERNIGHT SCRAPING:\n",
    "# Session 1: 2021-01-01 to 2022-06-30 (500 days)\n",
    "# Session 2: 2022-07-01 to 2024-01-01 (500 days)\n",
    "# Session 3: 2024-01-02 to 2025-10-09 (600 days)\n",
    "\n",
    "START_DATE = (2021, 1, 1)   # Change this per session\n",
    "END_DATE = (2022, 6, 30)     # Change this per session\n",
    "MAX_ARTICLES_PER_TOPIC = 25  # Increased for 20-30 articles/day\n",
    "\n",
    "print(f\"✅ Configuration:\")\n",
    "print(f\"   Topics: {len(TOPICS)}\")\n",
    "print(f\"   Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"   Max per topic: {MAX_ARTICLES_PER_TOPIC}\")\n",
    "print(f\"   Expected: {len(TOPICS) * MAX_ARTICLES_PER_TOPIC} articles per scrape\")\n",
    "print(f\"\\n⚠️  REMEMBER: Change START_DATE and END_DATE for each session!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650e11c",
   "metadata": {},
   "source": [
    "## 5️⃣ Run the Scraper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a914b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "!mkdir -p scraped_news\n",
    "\n",
    "# Initialize scraper with auto-save every 10 articles\n",
    "scraper = NewsScraperColab(checkpoint_interval=10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🚀 STARTING PRODUCTION NEWS SCRAPER\")\n",
    "print(\"=\"*70)\n",
    "print(f\"📅 Date Range: {START_DATE} to {END_DATE}\")\n",
    "print(f\"💾 Auto-save: Every 10 articles\")\n",
    "print(f\"🔄 Resume: Will skip cached URLs\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = {}\n",
    "all_articles = []\n",
    "\n",
    "try:\n",
    "    for topic in TOPICS:\n",
    "        # Scrape topic with date range\n",
    "        articles = scraper.scrape_topic(\n",
    "            topic, \n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            max_articles=MAX_ARTICLES_PER_TOPIC\n",
    "        )\n",
    "        \n",
    "        if articles:\n",
    "            # Save final topic file\n",
    "            filename = f\"scraped_news/{topic.replace(' ', '_').lower()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            \n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(articles, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            all_results[topic] = len(articles)\n",
    "            all_articles.extend(articles)\n",
    "            print(f\"💾 Final save: {filename}\")\n",
    "        \n",
    "        # Pause between topics\n",
    "        time.sleep(5)\n",
    "\n",
    "finally:\n",
    "    scraper.cleanup()\n",
    "\n",
    "# Save combined file\n",
    "combined_file = f\"scraped_news/combined_{START_DATE[0]}_{START_DATE[1]:02d}_{START_DATE[2]:02d}_to_{END_DATE[0]}_{END_DATE[1]:02d}_{END_DATE[2]:02d}.json\"\n",
    "with open(combined_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_articles, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ SCRAPING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📊 Results by Topic:\")\n",
    "total = 0\n",
    "for topic, count in all_results.items():\n",
    "    print(f\"   {topic}: {count} articles\")\n",
    "    total += count\n",
    "\n",
    "print(f\"\\n🎯 Total Articles: {total}\")\n",
    "print(f\"💾 Combined file: {combined_file}\")\n",
    "print(f\"📂 Individual files: scraped_news/\")\n",
    "print(\"\\n💡 Files saved with checkpoints - crash-proof!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87b606",
   "metadata": {},
   "source": [
    "## 6️⃣ Create ZIP for Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbaf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ZIP file\n",
    "!zip -r scraped_news.zip scraped_news/\n",
    "!zip -u scraped_news.zip checkpoint_*.json scraped_cache.json\n",
    "\n",
    "print(\"\\n✅ ZIP file created: scraped_news.zip\")\n",
    "print(\"\\n📥 DOWNLOAD STEPS:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Click the 📁 folder icon (left sidebar)\")\n",
    "print(\"2. Find 'scraped_news.zip'\")\n",
    "print(\"3. Right-click → Download\")\n",
    "print(\"4. On your PC, extract to: data/raw/news/\")\n",
    "print(\"\\n🔄 TO CONTINUE THIS SESSION LATER:\")\n",
    "print(\"1. Download scraped_cache.json\")\n",
    "print(\"2. Upload it before running cell 5 again\")\n",
    "print(\"3. Scraper will skip already-scraped articles!\")\n",
    "print(\"\\n🎯 NEXT STEPS ON YOUR PC:\")\n",
    "print(\"   python src/scraping/2_merge_data.py  # Removes duplicates\")\n",
    "print(\"   python src/processing/parallel_summarizer.py\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8239b05",
   "metadata": {},
   "source": [
    "## 7️⃣ Preview Scraped Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fff05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample article\n",
    "import os\n",
    "\n",
    "json_files = [f for f in os.listdir('scraped_news') if f.endswith('.json')]\n",
    "\n",
    "if json_files:\n",
    "    with open(f'scraped_news/{json_files[0]}', 'r', encoding='utf-8') as f:\n",
    "        sample = json.load(f)\n",
    "    \n",
    "    print(\"📰 Sample Article:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Title: {sample[0]['title']}\")\n",
    "    print(f\"Publisher: {sample[0]['publisher']}\")\n",
    "    print(f\"Word Count: {sample[0]['word_count']}\")\n",
    "    print(f\"\\nBody Preview: {sample[0]['body'][:300]}...\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"No articles found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a37d5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Done!\n",
    "\n",
    "**What you have:**\n",
    "- ✅ All scraped articles in JSON format\n",
    "- ✅ ZIP file ready to download\n",
    "- ✅ No cost to you (free Colab compute!)\n",
    "\n",
    "**Next steps on your PC:**\n",
    "1. Extract `scraped_news.zip` → `data/raw/news/`\n",
    "2. Merge: `python src/scraping/2_merge_data.py`\n",
    "3. Summarize: `python src/processing/summarizer.py`\n",
    "4. FinBERT sentiment analysis (Day 5)\n",
    "\n",
    "---\n",
    "\n",
    "**💡 Pro Tip:** Run this notebook weekly to get fresh news data!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
